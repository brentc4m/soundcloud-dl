#!/usr/bin/python
from optparse import OptionParser
import cookielib
import urllib2
import random
import time
import sys
import re
import shutil
import json
import HTMLParser

# max number of download attempts
max_retry = 3

# set up header values and openers
header_values =  {'User-Agent' : 'Mozilla/5.0 (X11; U; Linux i686 (x86_64); en-US; rv:1.9.2.16) Gecko/20110319 Firefox/3.6.16', 'Accept' : 'application/xml,application/xhtml+xml,text/html;q=0.9,text/plain;q=0.8,image/png,*/*;q=0.5', 'Accept-Charset' : 'ISO-8859-1,utf-8;q=0.7,*;q=0.3', 'Accept-Language' : 'en-US,en;q=0.8', 'Cache-Control' : 'max-age=0', 'Connection' : 'keep-alive'}
cj = cookielib.MozillaCookieJar()
opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj), urllib2.HTTPHandler())
urllib2.install_opener(opener)

# regexes
download_info_regex = re.compile(r'download-options.+')
download_path_regex = re.compile(r'href="(.+?)"')
song_info_regex = re.compile(r'bufferTracks.push\((.+?)\);')

# HTML parser
htmlparser = HTMLParser.HTMLParser()

def open_url(url):
    """ fetches html from given url """
    try:
        request = urllib2.Request(url, headers=header_values)
        response = opener.open(request)
    except urllib2.HTTPError, e:
        time.sleep(1)
    except ValueError, e:
        print str(e)
        return None
    html = response.read()
    return html

def get_song_info(page):
    info = {}
    download_match = download_info_regex.search(page)
    url_match = download_path_regex.search(download_match.group(0))
    song_info_match = song_info_regex.search(page, download_match.end())
    song_info = json.loads(song_info_match.group(1))
    if url_match:
        info['url'] = 'http://soundcloud.com' + url_match.group(1)
    else:
        info['url'] = song_info['streamUrl']
    info['title'] = re.sub(r'[\/:*?"<>|]', '', htmlparser.unescape(song_info['title']))
    return info

def get_soundcloud_links(url):
    """ given an url , scrape and return list of soundcloud links """
    retry = 0
    while True:
        if retry == max_retry:
            return None

        html = open_url(url)
        if not html:
            retry += 1
            print "could not fetch html. (%s) " % (retry)
            continue
        break
    return ['http://soundcloud.com' + url for url in re.findall('<h3><a href="(/.*?)">.*?</a></h3>', html)]

def download(song_info):
    """ given url with token and uid, download file to mp3 """

    # compose a url with uid and token and request the mpeg
    request = urllib2.Request(song_info['url'], headers=header_values)
    response = opener.open(request)

    f = open(song_info['title'] + '.mp3', 'w')
    shutil.copyfileobj(response, f)
    f.close()

def main(**kwargs):
    """ takes in an url or url to page to scrape soundcloud links """
    
    url = kwargs['url']

    print 'Downloading ' + url

    retry = 0
    while True:

        if retry == max_retry:
            print "failed to download song"
            sys.exit(1)

        # open up initial page to get stream token, uid, song title
        html = open_url(url)
        if not html:
            retry += 1
            print "Could not retrieve initial html. (%s) " % (retry)
            continue 
        
        song_info = get_song_info(html)
        break 

    # the browser does this...so we will too
    open_url('http://media.soundcloud.com/crossdomain.xml')

    download(song_info)
    print song_info['title'] + " successfully downloaded."

if __name__ == '__main__':

    parser = OptionParser()
    parser.add_option("-u", "--url", help="soundcloud url to download", dest="url")
    parser.add_option("-p", "--page", help="downloads all soundcloud urls found in given page", dest="page_url")
    parser.add_option("-f", "--favorites", help="downloads all favorites for the given soundcloud username", dest="favorites_user")
    parser.add_option("-t", "--track-file", help="records downloaded tracks and skips those already finished", dest="track_file")
    (options, args) = parser.parse_args()

    urls = set()
    history = None
    if options.page_url:
        urls.update(get_soundcloud_links(options.page_url))
    if options.url:
        urls.add(options.url) 
    if options.favorites_user:
        print 'Retrieving favorites, this may take a while..'
        fav_url = 'http://soundcloud.com/' + options.favorites_user + '/favorites'
        page = 1
        while True:
            favorites = get_soundcloud_links('%s?page=%d' % (fav_url, page))
            if favorites:
                urls.update(favorites)
                print 'Finished page %d' % page
                page += 1
            else:
                print 'All favorites found'
                break
    if options.track_file:
        history = open(options.track_file, 'a+')
        history.seek(0)
        finished_urls = [url.rstrip() for url in history.readlines()]
        urls.difference_update(finished_urls)
    if not options.url and not options.page_url and not options.favorites_user:
        parser.print_usage()

    try:
        for url in urls:
            main(**{'url':url})
            if history:
                history.write(url + '\n')
    finally:
        if history:
            history.close()
